# ceiba-crawler
ceiba-crawler

會想這麼做的原因是因為以往在使用ceiba時，使用上並不是十分方便，在尋找各個科目的相關作業、課程內容、相關講義時，需要相當頻繁地切換視窗，我們希望能將全部統合起來到一個視窗中；又或者是各科作業的繳交時間，我們希望能用更一目瞭然的方式呈現。
我們將從ceiba上獲取資料，將資料結合到google日曆中，在每天的日曆上，我們會呈現當天的課表和教授在課程內容公布當天所需的講義的相關連結；而我們也會將每次作業的繳交日期放到日曆上，附上該次作業的相關連結，並以與課表不同的顏色呈現，更具提醒效果；而在作業解答公佈時，也會結合到日曆中；最後我們希望能紀錄已經使用過這個程式的帳號，等到已經被記錄過的帳號再次使用這個程式時，只要更新之前還沒下載過的資料就好了，可以節省程式執行的時間，在使用上更有效率。
在編寫程式上，我們主要分成3個步驟：
1.	從ceiba上獲取資料，也就是crawling。在這個階段中我們主要使用selenium
模組，selenium為一自動化瀏覽器，雖然其原始的功能為自動化網站測試，但最近也越來越多人將之用來作為爬蟲。選擇selenium而不使用如scrapy或是requests+beautifulSoup的原因主要有二，其一是selenium相對於scrapy更容易安裝，使用上也更為直覺，不需要太多額外的知識；其二為selenium因為是自動化的瀏覽器，所以能很輕鬆的處理各種透過javascript驅動的模板、按鍵，但無論是scrapy或requests+beautifulSoup在面對javascript時都會遇上許多困難，增加開發時的困難。當然，相對於其他實作方法，selenium的效率差非常多，可是考量到我們的目的，即時性並不是我們首要的需求，1~2分鐘甚至5分鐘應該都還是可以接收的範圍。所以我們選擇使用selenium作為我們爬蟲的實作方法。
2.	從得到的資料中整理出我們想要的訊息，也就是parsing。在parsing的階段
我們還是透過beautifulSoup解析crawling得到的網頁原始碼。同時透過內建的datetime模組處理時間相關的資料，最後將所有的資料按照google calendar api規定的格式打包進dict裡並回傳。
3.	透過google calendar api將我們整理過的資料上傳，與google日曆結合。基
本上是對documentation上的範例作微調。流程為透過OAuth2(google提供的認證方法)取得憑證，並建立endpoint然後對日曆裡的事件、日曆列表等等做操作。
